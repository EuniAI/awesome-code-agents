- title: "MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test Multimodal Reasoning"
  authors: "Zikui Cai, Andrew Wang, Anirudh Satheesh, Ankit Nakhawa, Hyunwoo Jae, Keenan Powell, Minghui Liu, Neel Jay, Sungbin Oh, Xiyao Wang, Yongyuan Liang, Tom Goldstein, Furong Huang"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2506.05523"
    github: "https://github.com/morse-benchmark/morse-500"
    website: "https://morse-500.github.io/"

- title: "Chain-of-Modality: Learning Manipulation Programs from Multimodal Human Videos with Vision-Language-Models"
  authors: "Chen Wang, Fei Xia, Wenhao Yu, Tingnan Zhang, Ruohan Zhang, C. Karen Liu, Li Fei-Fei, Jie Tan, Jacky Liang"
  venue: "ICRA 2025"
  links:
    paper: "https://arxiv.org/abs/2504.13351"
    github: ""
    website: "https://chain-of-modality.github.io/"

- title: "Visual Agentic AI for Spatial Reasoning with a Dynamic API"
  authors: "Damiano Marsili, Rohun Agrawal, Yisong Yue, Georgia Gkioxari"
  venue: "CVPR 2025"
  links:
    paper: "https://openaccess.thecvf.com/content/CVPR2025/html/Marsili_Visual_Agentic_AI_for_Spatial_Reasoning_with_a_Dynamic_API_CVPR_2025_paper.html"
    github: "https://github.com/damianomarsili/VADAR"
    website: "https://glab-caltech.github.io/vadar/"

- title: "Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection"
  authors: "Enshen Zhou, Qi Su, Cheng Chi, Zhizheng Zhang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, He Wang"
  venue: "CVPR 2025"
  links:
    paper: "https://arxiv.org/abs/2412.04455"
    github: ""
    website: "https://zhoues.github.io/Code-as-Monitor/"

- title: "GRS: Generating Robotic Simulation Tasks from Real-World Images"
  authors: "Alex Zook, Fan-Yun Sun, Josef Spjut, Valts Blukis, Stan Birchfield, Jonathan Tremblay"
  venue: "CVPR 2025"
  links:
    paper: "https://openaccess.thecvf.com/content/CVPR2025W/CV2/html/Zook_GRS_Generating_Robotic_Simulation_Tasks_from_Real-World_Images_CVPRW_2025_paper.html"
    github: ""
    website: ""

- title: "IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering"
  authors: "Parker Liu, Chenxin Li, Zhengxin Li, Yipeng Wu, Wuyang Li, Zhiqin Yang, Zhenyuan Zhang, Yunlong Lin, Sirui Han, Brandon Y. Feng"
  venue: "NeurIPS 2025 Datasets and Benchmarks Track"
  links:
    paper: "https://arxiv.org/abs/2506.23329"
    github: "https://github.com/LiuHengyu321/IR3D-Bench"
    website: "https://ir3d-bench.github.io/"

- title: "Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation"
  authors: "Yue Yang, Ajay Patel, Matt Deitke, Tanmay Gupta, Luca Weihs, Andrew Head, Mark Yatskar, Chris Callison-Burch, Ranjay Krishna, Aniruddha Kembhavi, Christopher Clark"
  venue: "ACL 2025"
  links:
    paper: "https://arxiv.org/abs/2502.14846"
    github: ""
    website: ""

- title: "WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment"
  authors: "Hao Tang, Darren Yan Key, Kevin Ellis"
  venue: "NeurIPS 2024"
  links:
    paper: "https://openreview.net/forum?id=QGJSXMhVaL"
    github: "https://github.com/haotang1995/WorldCoder"
    website: "https://haotang1995.github.io/projects/worldcoder"

- title: "RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis"
  authors: "Yao Mu, Junting Chen, Qinglong Zhang, Shoufa Chen, Qiaojun Yu, Chongjian Ge, Runjian Chen, Zhixuan Liang, Mengkang Hu, Chaofan Tao, Peize Sun, Haibao Yu, Chao Yang, Wenqi Shao, Wenhai Wang, Jifeng Dai, Yu Qiao, Mingyu Ding, Ping Luo"
  venue: "ICML 2024"
  links:
    paper: "https://dl.acm.org/doi/10.5555/3692070.3693552"
    github: "https://github.com/RoboCodeX-source/RoboCodeX_code"
    website: "https://sites.google.com/view/robocodexplus"

- title: "RoboScript: Code Generation for Free-Form Manipulation Tasks across Real and Simulation"
  authors: "Junting Chen, Yao Mu, Qiaojun Yu, Tianming Wei, Silang Wu, Zhecheng Yuan, Zhixuan Liang, Chao Yang, Kaipeng Zhang, Wenqi Shao, Yu Qiao, Huazhe Xu, Mingyu Ding, Ping Luo"
  venue: "arXiv 2024"
  links:
    paper: "https://arxiv.org/abs/2402.14623"
    github: ""
    website: ""

- title: "ChatGPT for Robotics: Design Principles and Model Abilities"
  authors: "Sai Vemprala, Rogerio Bonatti, Arthur Bucker, Ashish Kapoor"
  venue: "arXiv 2023"
  links:
    paper: "https://arxiv.org/abs/2306.17582"
    github: "https://github.com/microsoft/PromptCraft-Robotics"
    website: "https://www.microsoft.com/en-us/research/articles/chatgpt-for-robotics/"

- title: "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models"
  authors: "Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg"
  venue: "ICRA 2023"
  links:
    paper: "https://arxiv.org/abs/2209.11302"
    github: "https://github.com/NVlabs/progprompt-vh"
    website: "https://progprompt.github.io/"

- title: "Code as Policies: Language Model Programs for Embodied Control"
  authors: "Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng"
  venue: "ICRA 2023"
  links:
    paper: "https://arxiv.org/abs/2209.07753"
    github: "https://github.com/google-research/google-research/tree/master/code_as_policies"
    website: "https://code-as-policies.github.io/"

- title: "Visual Programming: Compositional visual reasoning without training"
  authors: "Tanmay Gupta, Aniruddha Kembhavi"
  venue: "CVPR 2023"
  links:
    paper: "https://arxiv.org/abs/2211.11559"
    github: ""
    website: ""

- title: "ViperGPT: Visual Inference via Python Execution for Reasoning"
  authors: "Dídac Surís, Sachit Menon, Carl Vondrick"
  venue: "ICCV 2023"
  links:
    paper: "https://arxiv.org/abs/2303.08128"
    github: ""
    website: ""

- title: "ViStruct: Visual Structural Knowledge Extraction via Curriculum Guided Code-Vision Representation"
  authors: "Yangyi Chen, Xingyao Wang, Manling Li, Derek Hoiem, Heng Ji"
  venue: "EMNLP 2023"
  links:
    paper: "https://arxiv.org/abs/2311.13258"
    github: ""
    website: ""

- title: "Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model"
  authors: "Siyuan Huang, Zhengkai Jiang, Hao Dong, Yu Qiao, Peng Gao, Hongsheng Li"
  venue: "arXiv 2023"
  links:
    paper: "https://arxiv.org/abs/2305.11176"
    github: "https://github.com/OpenGVLab/Instruct2Act"
    website: ""

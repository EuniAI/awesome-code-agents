- title: "Terminal-Bench: A Benchmark for AI Agents in Terminal Environments"
  authors: "The Terminal-Bench Team"
  venue: "2025"
  links:
    paper: ""
    github: "https://github.com/laude-institute/terminal-bench"
    website: "https://www.tbench.ai/"

- title: "SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?"
  authors: "Xiang Deng, Jeff Da, Edwin Pan, Yannis Yiming He, Charles Ide, Kanak Garg, Niklas Lauffer, Andrew Park, Nitin Pasari, Chetan Rane, Karmini Sampath, Maya Krishnan, Srivatsa Kundurthy, Sean Hendryx, Zifan Wang, Chen Bo Calvin Zhang, Noah Jacobson, Bing Liu, Brad Kenstler"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2509.16941"
    github: "https://github.com/scaleapi/SWE-bench_Pro-os"
    website: "https://scale.com/leaderboard/swe_bench_pro_public"

- title: "SWE-PolyBench: A multi-language benchmark for repository level evaluation of coding agents"
  authors: "Muhammad Shihab Rashid, Christian Bock, Yuan Zhuang, Alexander Buchholz, Tim Esler, Simon Valentin, Luca Franceschi, Martin Wistuba, Prabhu Teja Sivaprasad, Woo Jung Kim, Anoop Deoras, Giovanni Zappella, Laurent Callot"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2504.08703"
    github: "https://github.com/amazon-science/SWE-PolyBench"
    website: "https://amazon-science.github.io/SWE-PolyBench/"

- title: "SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in Real-World Applications"
  authors: "Jinyang Li, Xiaolong Li, Ge Qu, Per Jacobsson, Bowen Qin, Binyuan Hui, Shuzheng Si, Nan Huo, Xiaohan Xu, Yue Zhang, Ziwei Tang, Yuanshuai Li, Florensia Widjaja, Xintong Zhu, Feige Zhou, Yongfeng Huang, Yannis Papakonstantinou, Fatma Ozcan, Chenhao Ma, Reynold Cheng"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2506.18951"
    github: "https://github.com/bird-bench/BIRD-CRITIC-1"
    website: "https://bird-critic.github.io/"

- title: "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving"
  authors: "Daoguang Zan, Zhirong Huang, Wei Liu, Hanwu Chen, Linhao Zhang, Shulin Xin, Lu Chen, Qi Liu, Xiaojian Zhong, Aoyan Li, Siyao Liu, Yongsheng Xiao, Liangqiang Chen, Yuyu Zhang, Jing Su, Tianyu Liu, Rui Long, Kai Shen, Liang Xiang"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2504.02605"
    github: "https://github.com/multi-swe-bench/multi-swe-bench"
    website: "https://multi-swe-bench.github.io/"

- title: "SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?"
  authors: "Xinyi He, Qian Liu, Mingzhe Du, Lin Yan, Zhijie Fan, Yiming Huang, Zejian Yuan, Zejun Ma"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2507.12415"
    github: "https://github.com/swe-perf/swe-perf"
    website: "https://swe-perf.github.io"

- title: "SWE-rebench: An Automated Pipeline for Task Collection and Decontaminated Evaluation of Software Engineering Agents"
  authors: "Ibragim Badertdinov, Alexander Golubev, Maksim Nekrashevich, Anton Shevtsov, Simon Karasik, Andrei Andriushchenko, Maria Trofimova, Daria Litvintseva, Boris Yangel"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2505.20411"
    github: ""
    website: "https://swe-rebench.com"

- title: "SWE-Bench+: Enhanced Coding Benchmark for LLMs"
  authors: "Reem Aleithan, Haoran Xue, Mohammad Mahdi Mohajer, Elijah Nnorom, Gias Uddin, Song Wang"
  venue: "arXiv 2025"
  links:
    paper: "https://openreview.net/forum?id=R40rS2afQ3"
    github: ""
    website: ""

- title: "SWE-Effi: Re-Evaluating Software AI Agent System Effectiveness Under Resource Constraints"
  authors: "Zhiyu Fan, Kirill Vasilevski, Dayi Lin, Boyuan Chen, Yihao Chen, Zhiqing Zhong, Jie M. Zhang, Pinjia He, Ahmed E. Hassan"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2509.09853"
    github: "https://centre-for-software-excellence.github.io/SWE-Effi/"  
    website: "https://github.com/Centre-for-Software-Excellence/SWE-Effi"

- title: "SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks"
  authors: "Hwiwon Lee, Ziqi Zhang, Hanxiao Lu, Lingming Zhang"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2506.11791"
    github: "https://github.com/SEC-bench/SEC-bench"
    website: "https://sec-bench.github.io/"

- title: "SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios"
  authors: "Junkai Chen, Huihui Huang, Yunbo Lyu, Junwen An, Jieke Shi, Chengran Yang, Ting Zhang, Haoye Tian, Yikun Li, Zhenhao Li, Xin Zhou, Xing Hu, David Lo"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2509.22097"
    github: "https://github.com/iCSawyer/SecureAgentBench"
    website: ""

- title: "SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks"
  authors: "Pavel Adamenko, Mikhail Ivanov, Aidar Valeev, Rodion Levichev, Pavel Zadorozhny, Ivan Lopatin, Dmitry Babayev, Alena Fenogenova, Valentin Malykh"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2507.11059"
    github: "https://github.com/MERA-Evaluation/repotest"
    website: "https://mera-evaluation.github.io/demo-swe-mera/"

- title: "Auto-SWE-Bench: A Framework for the Scalable Generation of Software Engineering Benchmark from Open-Source Repositories"
  authors: "Anonymous Authors"
  venue: "2025"
  links:
    paper: "https://openreview.net/forum?id=Gxw1EDSm9S"
    github: ""
    website: ""

- title: "SWE-bench Goes Live!"
  authors: "Linghao Zhang, Shilin He, Chaoyun Zhang, Yu Kang, Bowen Li, Chengxing Xie, Junhao Wang, Maoquan Wang, Yufan Huang, Shengyu Fu, Elsie Nallipogu, Qingwei Lin, Yingnong Dang, Saravan Rajmohan, Dongmei Zhang"
  venue: "NeurIPS 2025 Datasets & Benchmarks Track"
  links:
    paper: "https://arxiv.org/abs/2505.23419"
    github: "https://github.com/microsoft/SWE-bench-Live"
    website: "https://swe-bench-live.github.io/"

- title: "SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?"
  authors: "Samuel Miserendino, Michele Wang, Tejal Patwardhan, Johannes Heidecke"
  venue: "ICML 2025"
  links:
    paper: "https://arxiv.org/abs/2502.12115"
    github: "https://github.com/openai/frontier-evals/tree/main/project/swelancer"
    website: "https://openai.com/index/swe-lancer/"

- title: "Automated Benchmark Generation for Repository-Level Coding Tasks"
  authors: "Konstantinos Vergopoulos, Mark Niklas Müller, Martin Vechev"
  venue: "ICML 2025"
  links:
    paper: "https://openreview.net/forum?id=qnE2m3pIAb"
    github: ""
    website: ""

- title: "CVE-Bench: A Benchmark for AI Agents’ Ability to Exploit Real-World Web Application Vulnerabilities"
  authors: "Yuxuan Zhu, Antony Kellermann, Dylan Bowman, Philip Li, Akul Gupta, Adarsh Danda, Richard Fang, Conner Jensen, Eric Ihli, Jason Benn, Jet Geronimo, Avi Dhir, Sudhit Rao, Kaicheng Yu, Twm Stone, Daniel Kang"
  venue: "ICML 2025"
  links:
    paper: "https://arxiv.org/abs/2503.17332"
    github: "https://github.com/uiuc-kang-lab/cve-bench"
    website: ""

- title: "CVE-Bench: Benchmarking LLM-based Software Engineering Agent’s Ability to Repair Real-World CVE Vulnerabilities"
  authors: "Peiran Wang, Xiaogeng Liu, Chaowei Xiao"
  venue: "NAACL 2025"
  links:
    paper: "https://aclanthology.org/2025.naacl-long.212/"
    github: ""
    website: ""

- title: "On the Impacts of Contexts on Repository-Level Code Generation"
  authors: "Nam Le Hai, Dung Manh Nguyen, Nghi D. Q. Bui"
  venue: "NAACL 2025 Findings"
  links:
    paper: "https://aclanthology.org/2025.findings-naacl.82/"
    github: "https://github.com/FSoft-AI4Code/RepoExec"
    website: "https://fsoft-ai4code.github.io/repoexec/"

- title: "EnvBench: A Benchmark for Automated Environment Setup"
  authors: "Aleksandra Eliseeva, Alexander Kovrigin, Ilia Kholkin, Egor Bogomolov, Yaroslav Zharov"
  venue: "ICLR 2025 Workshop"
  links:
    paper: "https://arxiv.org/abs/2503.14443"
    github: "https://github.com/JetBrains-Research/EnvBench"
    website: "https://huggingface.co/datasets/JetBrains-Research/EnvBench"

- title: "OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue Resolution"
  authors: "Lianghong Guo, Wei Tao, Runhan Jiang, Yanlin Wang, Jiachi Chen, Xilin Liu, Yuchi Ma, Mingzhi Mao, Hongyu Zhang, Zibin Zheng"
  venue: "ISSTA 2025"
  links:
    paper: "https://arxiv.org/abs/2505.04606"
    github: "https://github.com/DeepSoftwareAnalytics/OmniGIRL"
    website: "https://deepsoftwareanalytics.github.io/omnigirl_leaderboard.html"

- title: "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?"
  authors: "Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik Narasimhan"
  venue: "ICLR 2024"
  links:
    paper: "https://arxiv.org/abs/2310.06770"
    github: "https://github.com/SWE-bench/SWE-bench"
    website: "https://www.swebench.com/"

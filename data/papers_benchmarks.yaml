- title: "Terminal-Bench: A Benchmark for AI Agents in Terminal Environments"
  authors: "The Terminal-Bench Team"
  venue: "2025"
  links:
    paper: ""
    github: "https://github.com/laude-institute/terminal-bench"
    website: "https://www.tbench.ai/"

- title: "SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?"
  authors: "Xiang Deng, Jeff Da, Edwin Pan, Yannis Yiming He, Charles Ide, Kanak Garg, Niklas Lauffer, Andrew Park, Nitin Pasari, Chetan Rane, Karmini Sampath, Maya Krishnan, Srivatsa Kundurthy, Sean Hendryx, Zifan Wang, Chen Bo Calvin Zhang, Noah Jacobson, Bing Liu, Brad Kenstler"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2509.16941"
    github: "https://github.com/scaleapi/SWE-bench_Pro-os"
    website: "https://scale.com/leaderboard/swe_bench_pro_public"

- title: "SWE-Compass: Towards Unified Evaluation of Agentic Coding Abilities for Large Language Models"
  authors: "Jingxuan Xu, Ken Deng, Weihao Li, Songwei Yu, Huaixi Tang, Haoyang Huang, Zhiyi Lai, Zizheng Zhan, Yanan Wu, Chenchen Zhang, Kepeng Lei, Yifan Yao, Xinping Lei, Wenqiang Zhu, Zongxian Feng, Han Li, Junqi Xiong, Dailin Li, Zuchen Gao, Kun Wu, Wen Xiang, Ziqi Zhan, Yuanxing Zhang, Wuxuan Gong, Ziyuan Gao, Guanxiang Wang, Yirong Xue, Mengtong Li, Mengfei Xie, Xiaojiang Zhang, Jinghui Wang, Wenhao Zhuang, Zheng Lin, Huiming Wang, Zhaoxiang Zhang, Yuqun Zhang, Haotian Zhang, Bin Chen, Jiaheng Liu"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2511.05459"
    github: ""
    website: ""

- title: "SWE-PolyBench: A multi-language benchmark for repository level evaluation of coding agents"
  authors: "Muhammad Shihab Rashid, Christian Bock, Yuan Zhuang, Alexander Buchholz, Tim Esler, Simon Valentin, Luca Franceschi, Martin Wistuba, Prabhu Teja Sivaprasad, Woo Jung Kim, Anoop Deoras, Giovanni Zappella, Laurent Callot"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2504.08703"
    github: "https://github.com/amazon-science/SWE-PolyBench"
    website: "https://amazon-science.github.io/SWE-PolyBench/"

- title: "Programming with Pixels: Can Computer-Use Agents do Software Engineering?"
  authors: "Pranjal Aggarwal, Sean Welleck"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2502.18525"
    github: "https://github.com/ProgrammingwithPixels/PwP"
    website: "https://programmingwithpixels.com/"

- title: "MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test Multimodal Reasoning"
  authors: "Zikui Cai, Andrew Wang, Anirudh Satheesh, Ankit Nakhawa, Hyunwoo Jae, Keenan Powell, Minghui Liu, Neel Jay, Sungbin Oh, Xiyao Wang, Yongyuan Liang, Tom Goldstein, Furong Huang"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2506.05523"
    github: "https://github.com/morse-benchmark/morse-500"
    website: "https://morse-500.github.io/"

- title: "SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in Real-World Applications"
  authors: "Jinyang Li, Xiaolong Li, Ge Qu, Per Jacobsson, Bowen Qin, Binyuan Hui, Shuzheng Si, Nan Huo, Xiaohan Xu, Yue Zhang, Ziwei Tang, Yuanshuai Li, Florensia Widjaja, Xintong Zhu, Feige Zhou, Yongfeng Huang, Yannis Papakonstantinou, Fatma Ozcan, Chenhao Ma, Reynold Cheng"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2506.18951"
    github: "https://github.com/bird-bench/BIRD-CRITIC-1"
    website: "https://bird-critic.github.io/"

- title: "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving"
  authors: "Daoguang Zan, Zhirong Huang, Wei Liu, Hanwu Chen, Linhao Zhang, Shulin Xin, Lu Chen, Qi Liu, Xiaojian Zhong, Aoyan Li, Siyao Liu, Yongsheng Xiao, Liangqiang Chen, Yuyu Zhang, Jing Su, Tianyu Liu, Rui Long, Kai Shen, Liang Xiang"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2504.02605"
    github: "https://github.com/multi-swe-bench/multi-swe-bench"
    website: "https://multi-swe-bench.github.io/"

- title: "SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?"
  authors: "Xinyi He, Qian Liu, Mingzhe Du, Lin Yan, Zhijie Fan, Yiming Huang, Zejian Yuan, Zejun Ma"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2507.12415"
    github: "https://github.com/swe-perf/swe-perf"
    website: "https://swe-perf.github.io"

- title: "MetaGen: A DSL, Database, and Benchmark for VLM-Assisted Metamaterial Generation"
  authors: "Liane Makatura, Benjamin Jones, Siyuan Bian, Wojciech Matusik"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2508.17568"
    github: ""
    website: ""

- title: "WebDS: An End-to-End Benchmark for Web-based Data Science"
  authors: "Ethan Hsu, Hong Meng Yam, Ines Bouissou, Aaron Murali John, Raj Thota, Josh Koe, Vivek Sarath Putta, G K Dharesan, Alexander Spangher, Shikhar Murty, Tenghao Huang, Christopher D. Manning"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2508.01222"
    github: ""
    website: ""

- title: "Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots"
  authors: "Chengyue Wu, Yixiao Ge, Qiushan Guo, Jiahao Wang, Zhixuan Liang, Zeyu Lu, Ying Shan, Ping Luo"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2405.07990"
    github: "https://github.com/TencentARC/Plot2Code"
    website: "https://huggingface.co/datasets/TencentARC/Plot2Code"

- title: "SWE-rebench: An Automated Pipeline for Task Collection and Decontaminated Evaluation of Software Engineering Agents"
  authors: "Ibragim Badertdinov, Alexander Golubev, Maksim Nekrashevich, Anton Shevtsov, Simon Karasik, Andrei Andriushchenko, Maria Trofimova, Daria Litvintseva, Boris Yangel"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2505.20411"
    github: ""
    website: "https://swe-rebench.com"

- title: "SWE-Bench+: Enhanced Coding Benchmark for LLMs"
  authors: "Reem Aleithan, Haoran Xue, Mohammad Mahdi Mohajer, Elijah Nnorom, Gias Uddin, Song Wang"
  venue: "arXiv 2025"
  links:
    paper: "https://openreview.net/forum?id=R40rS2afQ3"
    github: ""
    website: ""

- title: "SWE-Effi: Re-Evaluating Software AI Agent System Effectiveness Under Resource Constraints"
  authors: "Zhiyu Fan, Kirill Vasilevski, Dayi Lin, Boyuan Chen, Yihao Chen, Zhiqing Zhong, Jie M. Zhang, Pinjia He, Ahmed E. Hassan"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2509.09853"
    github: "https://centre-for-software-excellence.github.io/SWE-Effi/"
    website: "https://github.com/Centre-for-Software-Excellence/SWE-Effi"

- title: "PReview: A Benchmark Dataset for Pull Request Outcomes and Quality Analysis"
  authors: "Anonymous Authors"
  venue: "2025"
  links:
    paper: "https://openreview.net/forum?id=cdwp8BXTVV"
    github: ""
    website: ""

- title: "SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks"
  authors: "Hwiwon Lee, Ziqi Zhang, Hanxiao Lu, Lingming Zhang"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2506.11791"
    github: "https://github.com/SEC-bench/SEC-bench"
    website: "https://sec-bench.github.io/"

- title: "SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios"
  authors: "Junkai Chen, Huihui Huang, Yunbo Lyu, Junwen An, Jieke Shi, Chengran Yang, Ting Zhang, Haoye Tian, Yikun Li, Zhenhao Li, Xin Zhou, Xing Hu, David Lo"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2509.22097"
    github: "https://github.com/iCSawyer/SecureAgentBench"
    website: ""

- title: "SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks"
  authors: "Pavel Adamenko, Mikhail Ivanov, Aidar Valeev, Rodion Levichev, Pavel Zadorozhny, Ivan Lopatin, Dmitry Babayev, Alena Fenogenova, Valentin Malykh"
  venue: "arXiv 2025"
  links:
    paper: "https://arxiv.org/abs/2507.11059"
    github: "https://github.com/MERA-Evaluation/repotest"
    website: "https://mera-evaluation.github.io/demo-swe-mera/"

- title: "Auto-SWE-Bench: A Framework for the Scalable Generation of Software Engineering Benchmark from Open-Source Repositories"
  authors: "Anonymous Authors"
  venue: "2025"
  links:
    paper: "https://openreview.net/forum?id=Gxw1EDSm9S"
    github: ""
    website: ""

- title: "Can Agents Fix Agent Issues?"
  authors: "Alfin Wijaya Rahardja, Junwei Liu, Weitong Chen, Zhenpeng Chen, Yiling Lou"
  venue: "NeurIPS 2025"
  links:
    paper: "https://arxiv.org/abs/2505.20749"
    github: "https://github.com/alfin06/AgentIssue-Bench"
    website: ""

- title: "SWE-bench Goes Live!"
  authors: "Linghao Zhang, Shilin He, Chaoyun Zhang, Yu Kang, Bowen Li, Chengxing Xie, Junhao Wang, Maoquan Wang, Yufan Huang, Shengyu Fu, Elsie Nallipogu, Qingwei Lin, Yingnong Dang, Saravan Rajmohan, Dongmei Zhang"
  venue: "NeurIPS 2025 Datasets & Benchmarks Track"
  links:
    paper: "https://arxiv.org/abs/2505.23419"
    github: "https://github.com/microsoft/SWE-bench-Live"
    website: "https://swe-bench-live.github.io/"

- title: "IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering"
  authors: "Parker Liu, Chenxin Li, Zhengxin Li, Yipeng Wu, Wuyang Li, Zhiqin Yang, Zhenyuan Zhang, Yunlong Lin, Sirui Han, Brandon Y. Feng"
  venue: "NeurIPS 2025 Datasets and Benchmarks Track"
  links:
    paper: "https://arxiv.org/abs/2506.23329"
    github: "https://github.com/LiuHengyu321/IR3D-Bench"
    website: "https://ir3d-bench.github.io/"

- title: "BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing"
  authors: "Yunqi Gu, Ian Huang, Jihyeon Je, Guandao Yang, Leonidas Guiba"
  venue: "CVPR 2025"
  links:
    paper: "https://openaccess.thecvf.com/content/CVPR2025/html/Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing_CVPR_2025_paper.html"
    github: "https://github.com/richard-guyunqi/BlenderGym-Open"
    website: "https://blendergym.github.io/"

- title: "SyncMind: Measuring Agent Out-of-Sync Recovery in Collaborative Software Engineering"
  authors: "Xuehang Guo, Xingyao Wang, Yangyi Chen, Sha Li, Chi Han, Manling Li, Heng Ji"
  venue: "ICML 2025"
  links:
    paper: "https://arxiv.org/abs/2502.06994"
    github: "https://github.com/xhguo7/SyncMind"
    website: "https://xhguo7.github.io/SyncMind/"

- title: "SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?"
  authors: "Samuel Miserendino, Michele Wang, Tejal Patwardhan, Johannes Heidecke"
  venue: "ICML 2025"
  links:
    paper: "https://arxiv.org/abs/2502.12115"
    github: "https://github.com/openai/frontier-evals/tree/main/project/swelancer"
    website: "https://openai.com/index/swe-lancer/"

- title: "Automated Benchmark Generation for Repository-Level Coding Tasks"
  authors: "Konstantinos Vergopoulos, Mark Niklas Müller, Martin Vechev"
  venue: "ICML 2025"
  links:
    paper: "https://openreview.net/forum?id=qnE2m3pIAb"
    github: ""
    website: ""

- title: "CVE-Bench: A Benchmark for AI Agents’ Ability to Exploit Real-World Web Application Vulnerabilities"
  authors: "Yuxuan Zhu, Antony Kellermann, Dylan Bowman, Philip Li, Akul Gupta, Adarsh Danda, Richard Fang, Conner Jensen, Eric Ihli, Jason Benn, Jet Geronimo, Avi Dhir, Sudhit Rao, Kaicheng Yu, Twm Stone, Daniel Kang"
  venue: "ICML 2025"
  links:
    paper: "https://arxiv.org/abs/2503.17332"
    github: "https://github.com/uiuc-kang-lab/cve-bench"
    website: ""

- title: "WebMMU: A Benchmark for Multimodal Multilingual Website Understanding and Code Generation"
  authors: "Rabiul Awal, Mahsa Massoud, Aarash Feizi, Zichao Li, Suyuchen Wang, Christopher Pal, Aishwarya Agrawal, David Vazquez, Siva Reddy, Juan A. Rodriguez, Perouz Taslakian, Spandana Gella, Sai Rajeswar"
  venue: "EMNLP 2025"
  links:
    paper: "https://arxiv.org/abs/2508.16763"
    github: ""
    website: "https://webmmu-paper.github.io/"

- title: "CVE-Bench: Benchmarking LLM-based Software Engineering Agent’s Ability to Repair Real-World CVE Vulnerabilities"
  authors: "Peiran Wang, Xiaogeng Liu, Chaowei Xiao"
  venue: "NAACL 2025"
  links:
    paper: "https://aclanthology.org/2025.naacl-long.212/"
    github: ""
    website: ""

- title: "On the Impacts of Contexts on Repository-Level Code Generation"
  authors: "Nam Le Hai, Dung Manh Nguyen, Nghi D. Q. Bui"
  venue: "NAACL 2025 Findings"
  links:
    paper: "https://aclanthology.org/2025.findings-naacl.82/"
    github: "https://github.com/FSoft-AI4Code/RepoExec"
    website: "https://fsoft-ai4code.github.io/repoexec/"

- title: "FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation for Feature Implementation"
  authors: "Wei Li, Xin Zhang, Zhongxin Guo, Shaoguang Mao, Wen Luo, Guangyue Peng, Yangyu Huang, Houfeng Wang, Scarlett Li"
  venue: "ACL 2025"
  links:
    paper: "https://arxiv.org/abs/2503.06680"
    github: "https://github.com/microsoft/FEA-Bench"
    website: "https://gmago-leway.github.io/fea-bench.github.io/"

- title: "ProjectEval: A Benchmark for Programming Agents Automated Evaluation on Project-Level Code Generation"
  authors: "Kaiyuan Liu, Youcheng Pan, Yang Xiang, Daojing He, Jing Li, Yexing Du, Tianrun Gao"
  venue: "ACL 2025 Findings"
  links:
    paper: "https://aclanthology.org/2025.findings-acl.1036/"
    github: "https://github.com/RyanLoil/ProjectEval/"
    website: ""

- title: "EnvBench: A Benchmark for Automated Environment Setup"
  authors: "Aleksandra Eliseeva, Alexander Kovrigin, Ilia Kholkin, Egor Bogomolov, Yaroslav Zharov"
  venue: "ICLR 2025 Workshop"
  links:
    paper: "https://arxiv.org/abs/2503.14443"
    github: "https://github.com/JetBrains-Research/EnvBench"
    website: "https://huggingface.co/datasets/JetBrains-Research/EnvBench"

- title: "OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue Resolution"
  authors: "Lianghong Guo, Wei Tao, Runhan Jiang, Yanlin Wang, Jiachi Chen, Xilin Liu, Yuchi Ma, Mingzhi Mao, Hongyu Zhang, Zibin Zheng"
  venue: "ISSTA 2025"
  links:
    paper: "https://arxiv.org/abs/2505.04606"
    github: "https://github.com/DeepSoftwareAnalytics/OmniGIRL"
    website: "https://deepsoftwareanalytics.github.io/omnigirl_leaderboard.html"

- title: "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?"
  authors: "Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik Narasimhan"
  venue: "ICLR 2024"
  links:
    paper: "https://arxiv.org/abs/2310.06770"
    github: "https://github.com/SWE-bench/SWE-bench"
    website: "https://www.swebench.com/"

- title: "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems"
  authors: "Tianyang Liu, Canwen Xu, Julian McAuley"
  venue: "ICLR 2024"
  links:
    paper: "https://openreview.net/forum?id=pPjZIOuQuF"
    github: "https://github.com/Leolty/repobench"
    website: ""

- title: "CADTalk: An Algorithm and Benchmark for Semantic Commenting of CAD Programs"
  authors: "Haocheng Yuan, Jing Xu, Hao Pan, Adrien Bousseau, Niloy J. Mitra, Changjian Li"
  venue: "CVPR 2024"
  links:
    paper: "https://openaccess.thecvf.com/content/CVPR2024/html/Yuan_CADTalk_An_Algorithm_and_Benchmark_for_Semantic_Commenting_of_CAD_CVPR_2024_paper.html"
    github: "https://github.com/YYYYYHC/CADTalk"
    website: "https://enigma-li.github.io/CADTalk/"

- title: "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models"
  authors: "Yiming Huang, Jianwen Luo, Yan Yu, Yitong Zhang, Fangyu Lei, Yifan Wei, Shizhu He, Lifu Huang, Xiao Liu, Jun Zhao, Kang Liu"
  venue: "EMNLP 2024"
  links:
    paper: "https://aclanthology.org/2024.emnlp-main.748/"
    github: "https://github.com/yiyihum/da-code"
    website: "https://da-code-bench.github.io/"

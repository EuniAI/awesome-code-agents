<div align="center">
  <h1>ğŸ¤– Awesome Code Agents</h1>

  <!-- Badges -->
  <a href="https://awesome.re">
    <img src="https://awesome.re/badge.svg" alt="Awesome">
  </a>
  <a href="https://img.shields.io/badge/PRs-Welcome-red">
    <img src="https://img.shields.io/badge/PRs-Welcome-red" alt="PRs Welcome">
  </a>
  <a href="https://img.shields.io/github/last-commit/EuniAI/awesome-code-agents?color=green">
    <img src="https://img.shields.io/github/last-commit/EuniAI/awesome-code-agents?color=green" alt="Last Commit">
  </a>
</div>

<!-- Optional teaser -->
<!--
<p align="center">
  <img src="assets/teaser.png" width="520px"/>
</p>
-->
<p align="center">
  A curated list of <b>products, benchmarks, and research papers</b> on <b>Code Agents</b>.
</p>

---

## Quick Navigation

- [ğŸš€ Products & Tools](#-products--tools)
- [ğŸ“Š Leaderboards & Benchmarks](#-leaderboards--benchmarks)
- [ğŸ“š Papers](#-papers)
  * [Surveys](#-surveys)
  * [Environment Building](#-environment-building)
  * [Issue Reproduction](#-issue-reproduction)
  * [Issue Localization](#-issue-localization)
  * [Issue Resolution](#-issue-resolution)
  * [Q&A](#-qa)
  * [PR & Review](#-pr--review)
  * [Feature Development](#-feature-development)
  * [Git Management](#-git-management)
  * [Performance Optimization](#-performance-optimization)
  * [Website Generation](#-website-generation)
  * [Post-Training](#-post-training)
  * [Test-time Scaling](#-test-time-scaling)
  * [Multimodal](#-multimodal)
  * [Data Synthesis](#-data-synthesis)
  * [Empirical Studies](#-empirical-studies)  
- [ğŸ¤ Contributing](#-contributing)
- [ğŸŒŸ Star History](#-star-history)
- [ğŸ™ Acknowledgements](#-acknowledgements)

---

## ğŸš€ Products & Tools
> Open-source systems, frameworks, and real-world developer assistants.

- **OpenHands** [![Star](https://img.shields.io/github/stars/All-Hands-AI/OpenHands?style=social&label=Star)](https://github.com/All-Hands-AI/OpenHands) Â· [Website](https://all-hands.dev)
- **Prometheus** [![Star](https://img.shields.io/github/stars/EuniAI/Prometheus?style=social&label=Star)](https://github.com/EuniAI/Prometheus) Â· [Website](https://euni.ai/)
- **Aider** [![Star](https://img.shields.io/github/stars/paul-gauthier/aider?style=social&label=Star)](https://github.com/paul-gauthier/aider)
- **AutoCodeRover** 
- **KAT-Coder**
- **Kiro**
- **Droids**

---

## ğŸ“Š Leaderboards & Benchmarks
> Standardized evaluation suites for SWE agents.

- **SWE-bench Verified** â€” strict evaluation with verified solutions.  
  [![Star](https://img.shields.io/github/stars/princeton-nlp/SWE-bench?style=social&label=Star)](https://github.com/princeton-nlp/SWE-bench) Â· [Leaderboard](https://swe-bench.github.io/)

- **SWE-bench Pro** â€” professional benchmark variant for real-world SWE tasks.  
  [Link](https://swe-bench.github.io/)
- **SWE-bench Live** â€” SWE-bench Goes Live! [Link](https://swe-bench-live.github.io/)
- SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?
- SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks
- OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue Resolution
- SPICE: An Automated SWE-Bench Labeling Pipeline for Issue Clarity, Test Coverage, and Effort Estimation
- **Terminal-Bench** â€” command-line reasoning & execution benchmark.  
- **OSWorld / WebArena** â€” realistic environments for agent evaluation.  

---

## ğŸ“š Papers
### ğŸ” Surveys

- Agents in Software Engineering: Survey, Landscape, and Vision
- **OS Agents Survey** (Dec 2024) â€” survey on MLLM-based OS/IDE agents.  

---

### ğŸ— Environment Building
> Papers describing new environments, IDE sandboxes, benchmarks, or agent playgrounds.

- R2E: Turning any Github Repository into a Programming Agent Environment
- R2E-Gym: Procedural Environment Generation and Hybrid Verifiers for Scaling Open-Weights SWE Agents
- RepoST: Scalable Repository-Level Coding Environment Construction with Sandbox Testing
- You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary Projects
- Automated Benchmark Generation for Repository-Level Coding Tasks
- EnvBench: A Benchmark for Automated Environment Setup
- Treefix: Enabling Execution with a Tree of Prefixes
- CompileAgent: Automated Real-World Repo-Level Compilation with Tool-Integrated LLM-based Agent System
- AutoDev: Automated AI-Driven Development
- CXXCrafter: An LLM-Based Agent for Automated C/C++ Open Source Software Building
- CSR-Bench: Benchmarking LLM Agents in Deployment of Computer Science Research Repositories
- Automatically Generating Dockerfiles via Deep Learning: Challenges and Promises
- Beyond pip Install: Evaluating LLM Agents for the Automated Installation of Python Projects
- Repo2Run: Automated Building Executable Environment for Code Repository at Scale

### ğŸ” Issue Reproduction
> Research on reproducing software bugs deterministically.

- Issue2Test: Generating Reproducing Test Cases from Issue Reports

### ğŸ¯ Issue Localization
> Code search, fault localization, vulnerability detection.
- LocAgent: Graph-Guided LLM Agents for Code Localization
- ToolTrain: Tool-integrated Reinforcement Learning for Repo Deep Search
- CoSIL: Software Issue Localization via LLM-Driven Code Repository Graph Searching

### ğŸ›  Issue Resolution
> Automated bug fixing, patch generation, repair techniques.

- SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering
- AutoCodeRover: Autonomous Program Improvement
- Prometheus: Unified Knowledge Graphs for Issue Resolution in Multilingual Codebases
- debug-gym: A Text-Based Environment for Interactive Debugging
- SemAgent: A Semantics-Aware Program Repair Agent
- CodeR: Issue Resolving with Multi-Agent and Task Graphs
- Code Graph Model (CGM): A Graph-Integrated Large Language Model for Repository-Level Software Engineering Tasks
- SWE-Exp: Experience-Driven Software Issue Resolution
- SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning with LLM-Based Agents
- SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution
- Co-PatcheR: Collaborative Software Patching with Component(s)-specific Small Reasoning Models
- A Self-Improving Coding Agent
- SWE-Bench-CL: Continual Learning for Coding Agents
- Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling
- SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning
- Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning

### â“ Q&A
> Code understanding, documentation, and retrieval-based Q&A.

- SWE-QA: Can Language Models Answer Repository-level Code Questions?

### ğŸ” PR & Review
> Automated pull request creation, review assistance, linting, refactoring.  

### âœ¨ Feature Development
> Studies on agent-driven feature extension, repo-level edits.

- SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development
- NoCode-bench: A Benchmark for Evaluating Natural Language-Driven Feature Addition

### ğŸ”„ Git Management
> Agents for git workflows (branching, rebasing, conflict resolution).  

- GitGoodBench: A Novel Benchmark For Evaluating Agentic Performance On Git

### âš¡ Performance Optimization
> Code profiling, optimization, memory & latency improvements.

- SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?

### ğŸŒ Website Generation
> Code agents that generate or maintain websites/frontends.

- WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch
- ScreenCoder: Advancing Visual-to-Code Generation for Front-End Automation via Modular Multimodal Agents

### ğŸ•¹ï¸ Unified Agents
> Unified Software Engineering agent as AI Software Engineer

### ğŸ“ Post-Training
> Instruction tuning, alignment, reinforcement learning for SWE agents.

- Training Software Engineering Agents and Verifiers with SWE-Gym
- SEAlign: Alignment Training for Software Engineering Agent

### âš– Test-time Scaling
> Chain-of-thought, self-reflection, scaling strategies during inference.  

- SWE-Search: Enhancing Software Agents with Monte Carlo Tree Search and Iterative Refinement
- Thinking Longer, Not Larger: Enhancing Software Engineering Agents via Scaling Test-Time Compute

### ğŸ–¼ Multimodal
> Agents that leverage images/screenshots/GUI for coding tasks.

- Seeing is Fixing: Cross-Modal Reasoning with Multimodal LLMs for Visual Software Issue Fixing

### ğŸ§¬ Data Synthesis
> Synthetic data generation for code tasks, self-play, augmentation.

- MCTS-Refined CoT: High-Quality Fine-Tuning Data for LLM-Based Repository Issue Resolution
- SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner
- SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks

### ğŸ“Š Empirical Studies
> Evaluations of LLMs/agents on large-scale software repositories. Analysis of failure cases, bias, reproducibility.  

- Understanding Software Engineering Agents Through the Lens of Traceability: An Empirical Study
- Can LLMs Replace Manual Annotation of Software Engineering Artifacts?
- Understanding Software Engineering Agents: A Study of Thought-Action-Result Trajectories

---

## ğŸ¤ Contributing
We welcome contributions! Please:  
1. Use the [entry template](#entry-template).  
2. Place items in the right category & order by **reverse-chronology**.  
3. Include badges for GitHub stars, arXiv, website if available.

---

## ğŸŒŸ Star History
[![Star History Chart](https://api.star-history.com/svg?repos=EuniAI/awesome-code-agents&type=Date)](https://www.star-history.com/#EuniAI/awesome-code-agents&Date)

---

## ğŸ™ Acknowledgements
- Thanks to all contributors and the research community.
